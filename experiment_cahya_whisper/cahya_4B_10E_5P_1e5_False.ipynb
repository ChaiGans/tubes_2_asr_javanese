{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645211a8-e391-4bdf-9529-923b3872ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Whisper seq2seq training pipeline reading precomputed splits from sorted_data.csv.\n",
    "- sorted_data.csv expected columns: SentenceID, Transcript, Gender, Native, AudioPath, Split (train/val/test).\n",
    "- No in-notebook splitting; CSV controls reproducible splits.\n",
    "Note: This is a template; adjust model_name/checkpoint and hyperparams as needed.\n",
    "\"\"\"\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import torchaudio.functional as F\n",
    "from typing import List, Dict, Tuple\n",
    "import sys\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Hugging Face Whisper\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42901de9-2050-4318-b022-00e8b3a3338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(filepath, target_sr=16000):\n",
    "    # Read audio using soundfile (no torchcodec)\n",
    "    waveform_np, sr = sf.read(filepath, dtype=\"float32\")  # shape: (num_samples,) or (num_samples, channels)\n",
    "\n",
    "    # Convert to torch tensor and ensure shape (channels, num_samples)\n",
    "    waveform = torch.from_numpy(waveform_np)\n",
    "    if waveform.ndim == 1:\n",
    "        # mono\n",
    "        waveform = waveform.unsqueeze(0)  # (1, num_samples)\n",
    "    elif waveform.ndim == 2:\n",
    "        # (num_samples, channels) -> (channels, num_samples)\n",
    "        waveform = waveform.transpose(0, 1)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != target_sr:\n",
    "        waveform = F.resample(waveform, sr, target_sr)\n",
    "        sr = target_sr\n",
    "\n",
    "    return waveform, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf2c258-cc95-471d-bd32-be9b21bd2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------\n",
    "# Dataset / collator + metrics\n",
    "# ----------------------\n",
    "\n",
    "try:\n",
    "    import editdistance  # optional for speed\n",
    "except ImportError:\n",
    "    editdistance = None\n",
    "\n",
    "\n",
    "def _levenshtein(seq_a, seq_b):\n",
    "    dp = list(range(len(seq_b) + 1))\n",
    "    for i, ca in enumerate(seq_a, 1):\n",
    "        prev = dp[0]\n",
    "        dp[0] = i\n",
    "        for j, cb in enumerate(seq_b, 1):\n",
    "            cur = dp[j]\n",
    "            dp[j] = min(\n",
    "                dp[j] + 1,          # deletion\n",
    "                dp[j - 1] + 1,      # insertion\n",
    "                prev + (ca != cb),  # substitution\n",
    "            )\n",
    "            prev = cur\n",
    "    return dp[-1]\n",
    "\n",
    "\n",
    "def _edit_distance(a, b):\n",
    "    if editdistance is not None:\n",
    "        return editdistance.eval(a, b)\n",
    "    return _levenshtein(a, b)\n",
    "\n",
    "\n",
    "def compute_cer(reference, hypothesis):\n",
    "    # Accept either a single reference/hypothesis pair (str) or lists of pairs and return mean CER\n",
    "    if isinstance(reference, list):\n",
    "        if not isinstance(hypothesis, list) or len(reference) != len(hypothesis):\n",
    "            raise ValueError('reference and hypothesis must be lists of the same length')\n",
    "        if len(reference) == 0:\n",
    "            return 0.0\n",
    "        return float(np.mean([compute_cer(r, h) for r, h in zip(reference, hypothesis)]))\n",
    "    if len(reference) == 0:\n",
    "        return 0.0 if len(hypothesis) == 0 else 1.0\n",
    "    distance = _edit_distance(reference, hypothesis)\n",
    "    return distance / len(reference)\n",
    "\n",
    "\n",
    "def compute_wer(reference, hypothesis):\n",
    "    # Accept either a single reference/hypothesis pair (str) or lists of pairs and return mean WER\n",
    "    if isinstance(reference, list):\n",
    "        if not isinstance(hypothesis, list) or len(reference) != len(hypothesis):\n",
    "            raise ValueError('reference and hypothesis must be lists of the same length')\n",
    "        if len(reference) == 0:\n",
    "            return 0.0\n",
    "        return float(np.mean([compute_wer(r, h) for r, h in zip(reference, hypothesis)]))\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    if len(ref_words) == 0:\n",
    "        return 0.0 if len(hyp_words) == 0 else 1.0\n",
    "    distance = _edit_distance(ref_words, hyp_words)\n",
    "    return distance / len(ref_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_loss(model, dloader: DataLoader, device: str) -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dloader, desc=\"Eval loss\", leave=False):\n",
    "            input_features = batch['input_features'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            loss = model(input_features=input_features, attention_mask=attention_mask, labels=labels).loss\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    return total_loss / max(1, n_batches)\n",
    "\n",
    "\n",
    "def run_eval(model, processor, dloader, split_name, device):\n",
    "    model.eval()\n",
    "    refs, hyps = [], []\n",
    "\n",
    "    decoder_prompt = processor.get_decoder_prompt_ids(\n",
    "        language=\"id\",\n",
    "        task=\"transcribe\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dloader, desc=f\"Eval {split_name}\", leave=False):\n",
    "            input_features = batch['input_features'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Repeat prompt for batch size\n",
    "            decoder_input_ids = torch.tensor(decoder_prompt[0]).unsqueeze(0)\n",
    "            decoder_input_ids = decoder_input_ids.repeat(input_features.size(0), 1).to(device)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_features=input_features,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids\n",
    "            )\n",
    "\n",
    "            transcripts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            labels = batch[\"labels\"]\n",
    "            label_ids = labels.clone()\n",
    "            label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "            ref_texts = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "            refs.extend(ref_texts)\n",
    "            hyps.extend(transcripts)\n",
    "\n",
    "    wer = compute_wer(refs, hyps)\n",
    "    cer = compute_cer(refs, hyps)\n",
    "    avg_score = 0.5 * (wer + cer)\n",
    "\n",
    "    print(f\"{split_name} WER: {wer:.4f} | CER: {cer:.4f} | Avg: {avg_score:.4f}\")\n",
    "    return wer, cer, avg_score\n",
    "\n",
    "class WhisperAudioDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        waveform, sr = load_audio(row['AudioPath'], target_sr=16000)\n",
    "        waveform = waveform.mean(0)\n",
    "        return {\n",
    "            'audio': waveform,\n",
    "            'sampling_rate': sr,\n",
    "            'text': row['Transcript'],\n",
    "            'sentence_id': row['SentenceID'],\n",
    "        }\n",
    "\n",
    "\n",
    "def make_collate_fn(processor: WhisperProcessor):\n",
    "    max_length = processor.feature_extractor.n_samples\n",
    "    def collate(batch: List[Dict]):\n",
    "        audios = [b['audio'].squeeze().cpu().numpy() for b in batch]\n",
    "        texts = [b['text'] for b in batch]\n",
    "        inputs = processor(\n",
    "            audios,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        labels = processor.tokenizer(\n",
    "            texts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "        )\n",
    "        labels['input_ids'][labels['input_ids'] == processor.tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            'input_features': inputs.input_features,\n",
    "            'attention_mask': inputs.attention_mask,\n",
    "            'labels': labels['input_ids'],\n",
    "        }\n",
    "    return collate\n",
    "\n",
    "\n",
    "def load_splits(csv_path: str = './sorted_data.csv') -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if 'Split' not in df.columns:\n",
    "        raise ValueError(\"sorted_data.csv missing 'Split' column. Re-run data_sorting.py to add Split labels.\")\n",
    "    train_df = df[df['Split'] == 'train'].copy()\n",
    "    val_df = df[df['Split'] == 'val'].copy()\n",
    "    test_df = df[df['Split'] == 'test'].copy()\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable params: {trainable/1e6:.2f}M / {total/1e6:.2f}M total\")\n",
    "    return trainable, total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7562f349-b633-48b2-b7ee-0d3146bdfbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    model_name: str = \"cahya/whisper-medium-id\"\n",
    "    batch_size: int = 4\n",
    "    num_epochs: int = 10\n",
    "    lr: float = 1e-5\n",
    "    seed: int = 42\n",
    "    patience: int = 5\n",
    "    use_lora: bool = False\n",
    "    lora_r: int = 32\n",
    "    lora_alpha: int = 64\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: list = None\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def train_and_eval(config: TrainConfig):\n",
    "    set_seed(config.seed)\n",
    "    start_time = time.time()\n",
    "    device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "\n",
    "    train_df, val_df, test_df = load_splits()\n",
    "    print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "    print(f\"Using device: {config.device} ({device_name})\")\n",
    "\n",
    "    processor = WhisperProcessor.from_pretrained(config.model_name)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(config.model_name)\n",
    "\n",
    "    if config.use_lora:\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=config.lora_r,\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            lora_dropout=config.lora_dropout,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        model = get_peft_model(model, lora_cfg)\n",
    "        model.print_trainable_parameters()\n",
    "    trainable_params, total_params = count_trainable_params(model)\n",
    "    model.to(config.device)\n",
    "\n",
    "    train_ds = WhisperAudioDataset(train_df)\n",
    "    val_ds = WhisperAudioDataset(val_df)\n",
    "    test_ds = WhisperAudioDataset(test_df)\n",
    "\n",
    "    collate_fn = make_collate_fn(processor)\n",
    "    train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_ds, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
    "    total_steps = len(train_loader) * config.num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\", leave=False)\n",
    "        for step, batch in enumerate(pbar, 1):\n",
    "            optimizer.zero_grad()\n",
    "            input_features = batch['input_features'].to(config.device)\n",
    "            attention_mask = batch['attention_mask'].to(config.device)\n",
    "            labels = batch['labels'].to(config.device)\n",
    "            loss = model(input_features=input_features, attention_mask=attention_mask, labels=labels).loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "            if step % 10 == 0 or step == 1:\n",
    "                pbar.set_postfix(loss=running_loss / step)\n",
    "        avg_loss = running_loss / max(1, len(train_loader))\n",
    "        train_losses.append(avg_loss)\n",
    "        val_loss = evaluate_loss(model, val_loader, config.device)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{config.num_epochs} | Train loss: {avg_loss:.4f} | Val loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= config.patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} (no val improvement for {config.patience} epochs). Best val loss {best_val_loss:.4f} at epoch {best_epoch}.\")\n",
    "                break\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Total train+eval time: {elapsed/60:.2f} min ({elapsed:.1f} s) on {device_name}\")\n",
    "    print(f\"Best val loss: {best_val_loss:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "    metrics = []\n",
    "    for epoch_idx, (tr_loss, vl_loss) in enumerate(zip(train_losses, val_losses), start=1):\n",
    "        metrics.append({\n",
    "            'epoch': epoch_idx,\n",
    "            'train_loss': tr_loss,\n",
    "            'val_loss': vl_loss,\n",
    "            'model': config.model_name,\n",
    "            'batch_size': config.batch_size,\n",
    "            'lr': config.lr,\n",
    "            'num_epochs': config.num_epochs,\n",
    "            'seed': config.seed,\n",
    "            'patience': config.patience,\n",
    "            'device': device_name,\n",
    "            'use_lora': config.use_lora,\n",
    "            'lora_r': config.lora_r,\n",
    "            'lora_alpha': config.lora_alpha,\n",
    "            'lora_dropout': config.lora_dropout,\n",
    "            'trainable_params': trainable_params,\n",
    "            'total_params': total_params,\n",
    "        })\n",
    "    csv_name = f\"metrics_{config.model_name.replace('/', '-')}\"         f\"_bs{config.batch_size}_lr{config.lr}_ep{config.num_epochs}_pat{config.patience}_lora{int(config.use_lora)}_seed{config.seed}.csv\"\n",
    "    pd.DataFrame(metrics).to_csv(csv_name, index=False)\n",
    "    print(f\"Saved per-epoch metrics to {csv_name}\" )\n",
    "\n",
    "    return train_losses, val_losses, processor, model, (train_loader, val_loader, test_loader), device_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e825de-39df-4ae0-900f-cb059e1a8701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1470 | Val: 210 | Test: 410\n",
      "Using device: cuda (NVIDIA GeForce RTX 5090)\n",
      "Trainable params: 762.32M / 763.86M total\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13312264e3bc437ab0f32f908f78f788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fea1d4cdcb4d0f98b462702505b9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval loss:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train loss: 2.8000 | Val loss: 0.5952\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4f463fd1a846ea82d6f0bfd464dc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dc9281fd5f47f9a0d1c74ede7d0bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval loss:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train loss: 0.2819 | Val loss: 0.4912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3faf7d5d76ce4636b35a04067a76a0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07d5a991ca74923ac0504c1cc43025c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval loss:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train loss: 0.0726 | Val loss: 0.4908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6165ee6ee1f44cfe9c09d6590faefb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95964e0359f746968ada4a8fa84c2b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval loss:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train loss: 0.0210 | Val loss: 0.4891\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433403964e194397adc073c69a803554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b201ea6da0346a287844ce1de958c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval loss:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train loss: 0.0100 | Val loss: 0.5122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edb658d6dd64e37b8b0c1cf891414b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3d83eafa074024bc7d7a4f11b098b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval loss:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train loss: 0.0068 | Val loss: 0.5144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9002646ab42f488181c7c0049c66e1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cb30cef16945249cf631511912df0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval loss:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train loss: 0.0039 | Val loss: 0.5188\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd233aba078349b09716263fad400774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad05e93ac49492b9f9bff7776c06153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval loss:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train loss: 0.0031 | Val loss: 0.5217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1e82a603ed4e49a69c266f8618d003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0060223fc054b7caa3aa9edb5b2540a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval loss:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train loss: 0.0027 | Val loss: 0.5235\n",
      "Early stopping at epoch 9 (no val improvement for 5 epochs). Best val loss 0.4891 at epoch 4.\n",
      "Total train+eval time: 27.78 min (1667.0 s) on NVIDIA GeForce RTX 5090\n",
      "Best val loss: 0.4891 at epoch 4\n",
      "Saved per-epoch metrics to metrics_cahya-whisper-medium-id_bs4_lr1e-05_ep10_pat5_lora0_seed42.csv\n"
     ]
    }
   ],
   "source": [
    "cfg = TrainConfig()\n",
    "train_losses, val_losses, processor, model, (train_loader, val_loader, test_loader), device_name = train_and_eval(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd291bc-65d1-4ed4-8358-8c6904627264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.800046626539172, 0.28189226713679405, 0.0726359526199066, 0.02102503912401912, 0.009957859395438825, 0.006828814909765598, 0.0038847813914838734, 0.0031048067248775624, 0.002682181714866918]\n"
     ]
    }
   ],
   "source": [
    "print(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd66153-5f83-47a8-bd36-56f4a8215dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43578f3fc44a44bba4aaedac7664fe0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    }
   ],
   "source": [
    "# Final test evaluation (run after training)\n",
    "model.generation_config.language = None\n",
    "model.generation_config.task = None\n",
    "test_wer, test_cer, test_avg = run_eval(model, processor, test_loader, \"Test\", cfg.device)\n",
    "print(f\"Test WER: {test_wer:.4f} | CER: {test_cer:.4f} | Avg: {test_avg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b1862-ce0f-4917-9932-8b19593ac80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on a random test audio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Reload test split\n",
    "_, _, test_df = load_splits()\n",
    "\n",
    "# Pick a random example\n",
    "sample = test_df.sample(n=1, random_state=random.randint(0, 10_000)).iloc[0]\n",
    "\n",
    "audio_path = Path(sample['AudioPath'])\n",
    "waveform, sr = load_audio(audio_path, target_sr=16000)\n",
    "waveform = waveform.mean(0)  # ensure mono\n",
    "\n",
    "# Load processor + model if not in memory\n",
    "if 'processor' not in globals():\n",
    "    processor = WhisperProcessor.from_pretrained(cfg.model_name)\n",
    "if 'model' not in globals():\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(cfg.model_name)\n",
    "\n",
    "# IMPORTANT: clean old generation config\n",
    "model.generation_config.language = None\n",
    "model.generation_config.task = None\n",
    "model.to(cfg.device)\n",
    "model.eval()\n",
    "\n",
    "# Build decoder prompt (new API)\n",
    "decoder_prompt = processor.get_decoder_prompt_ids(\n",
    "    language=\"id\",\n",
    "    task=\"transcribe\"\n",
    ")[0]\n",
    "\n",
    "inputs = processor(\n",
    "    waveform.cpu().numpy(),\n",
    "    sampling_rate=sr,\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    max_length=processor.feature_extractor.n_samples,\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "\n",
    "input_features = inputs.input_features.to(cfg.device)\n",
    "attention_mask = inputs.attention_mask.to(cfg.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoder_input_ids = torch.tensor(decoder_prompt).unsqueeze(0).to(cfg.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_features=input_features,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=decoder_input_ids\n",
    "    )\n",
    "    \n",
    "    pred_text = processor.batch_decode(\n",
    "        generated_ids, skip_special_tokens=True\n",
    "    )[0]\n",
    "\n",
    "display(Audio(str(audio_path), rate=sr))\n",
    "print(f\"Audio file: {audio_path}\")\n",
    "print(f\"Reference: {sample['Transcript']}\")\n",
    "print(f\"Prediction: {pred_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2c6a7-150b-4eef-88c7-714af40e99f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44215f11-1992-4728-a0ca-98e37c5a1c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vast venv",
   "language": "python",
   "name": "vast_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
