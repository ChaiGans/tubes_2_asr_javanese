
========================================================================================================================
EXPERIMENT SUMMARY (Scratch Deep Dive)
========================================================================================================================

QUICK OVERVIEW (sorted by WER):
----------------------------------------------------------------------------------------------------
ID   Name                                WER        CER        Time(m)   
----------------------------------------------------------------------------------------------------
9    V3-03: Word + Larger Model          0.9270     0.7211     9.4       
12   V3-06: Word + More Epochs           0.9291     0.7312     9.6       
10   V3-04: Word + CTC Joint Training    0.9403     0.7521     6.8       
2    S02: Word Vocab (Word, Pyr, LSTM)   0.9424     0.7210     6.4       
1    S01: Baseline (Char, Pyr, LSTM)     0.9473     0.7156     13.0      
7    V3-01: Word + Higher LR             0.9480     0.7641     6.4       
8    V3-02: Word + GRU Decoder           0.9522     0.7497     6.4       
4    S04: High LR (1e-3)                 0.9860     0.7911     12.7      
11   V3-05: Char + Lower Dropout         0.9937     0.7560     14.4      
3    S03: GRU Decoder (Char, Pyr, GRU)   0.9993     0.7413     12.5      
6    S06: Standard Enc (Char, Std, LSTM) 1.0014     0.8271     23.0      
5    S05: Low LR (1e-4)                  1.0927     0.8379     12.8      


DETAILED CONFIGURATIONS:
====================================================================================================

[Experiment 9] V3-03: Word + Larger Model
------------------------------------------------------------
  Model:
    token_type:      word
    encoder_type:    pyramidal
    decoder_type:    lstm
  Training:
    learning_rate:   0.0005
    num_epochs:      100
    batch_size:      32
    dropout:         0.3
  Results:
    best_val_wer:    0.9269662921348315
    final_val_cer:   0.7210765466620063
    training_time:   9.4 min

[Experiment 12] V3-06: Word + More Epochs
------------------------------------------------------------
  Model:
    token_type:      word
    encoder_type:    pyramidal
    decoder_type:    lstm
  Training:
    learning_rate:   0.0005
    num_epochs:      150
    batch_size:      32
    dropout:         0.3
  Results:
    best_val_wer:    0.9290730337078652
    final_val_cer:   0.7312128626354422
    training_time:   9.6 min

[Experiment 10] V3-04: Word + CTC Joint Training
------------------------------------------------------------
  Model:
    token_type:      word
    encoder_type:    pyramidal
    decoder_type:    lstm
  Training:
    learning_rate:   0.0005
    num_epochs:      100
    batch_size:      32
    dropout:         0.3
  Results:
    best_val_wer:    0.9403089887640449
    final_val_cer:   0.752068041477339
    training_time:   6.8 min

[Experiment 2] S02: Word Vocab (Word, Pyr, LSTM)
------------------------------------------------------------
  Model:
    token_type:      word
    encoder_type:    pyramidal
    decoder_type:    lstm
  Training:
    learning_rate:   0.0005
    num_epochs:      100
    batch_size:      32
    dropout:         0.3
  Results:
    best_val_wer:    0.9424157303370787
    final_val_cer:   0.7209600372830013
    training_time:   6.4 min

[Experiment 1] S01: Baseline (Char, Pyr, LSTM)
------------------------------------------------------------
  Model:
    token_type:      char
    encoder_type:    pyramidal
    decoder_type:    lstm
  Training:
    learning_rate:   0.0005
    num_epochs:      100
    batch_size:      32
    dropout:         0.3
  Results:
    best_val_wer:    0.9473314606741573
    final_val_cer:   0.7156006058487708
    training_time:   13.0 min

[Experiment 7] V3-01: Word + Higher LR
------------------------------------------------------------
  Model:
    token_type:      word
    encoder_type:    pyramidal
    decoder_type:    lstm
  Training:
    learning_rate:   0.001
    num_epochs:      100
    batch_size:      32
    dropout:         0.3
  Results:
    best_val_wer:    0.9480337078651685
    final_val_cer:   0.764068507514855
    training_time:   6.4 min

[Experiment 8] V3-02: Word + GRU Decoder
------------------------------------------------------------
  Model:
    token_type:      word
    encoder_type:    pyramidal
    decoder_type:    gru
  Training:
    learning_rate:   0.0005
    num_epochs:      100
    batch_size:      32
    dropout:         0.3
  Results:
    best_val_wer:    0.952247191011236
    final_val_cer:   0.7497378538972387
    training_time:   6.4 min

[Experiment 4] S04: High LR (1e-3)
------------------------------------------------------------
  Model:
    token_type:      char
    encoder_type:    pyramidal
    decoder_type:    lstm
  Training:
    learning_rate:   0.001
    num_epochs:      100
    batch_size:      32
    dropout:         0.3
  Results:
    best_val_wer:    0.9859550561797753
    final_val_cer:   0.7910986834440172
    training_time:   12.7 min

[Experiment 11] V3-05: Char + Lower Dropout
------------------------------------------------------------
  Model:
    token_type:      char
    encoder_type:    pyramidal
    decoder_type:    lstm
  Training:
    learning_rate:   0.0005
    num_epochs:      100
    batch_size:      32
    dropout:         0.1
  Results:
    best_val_wer:    0.9936797752808989
    final_val_cer:   0.7560293603635093
    training_time:   14.4 min

[Experiment 3] S03: GRU Decoder (Char, Pyr, GRU)
------------------------------------------------------------
  Model:
    token_type:      char
    encoder_type:    pyramidal
    decoder_type:    gru
  Training:
    learning_rate:   0.0005
    num_epochs:      100
    batch_size:      32
    dropout:         0.3
  Results:
    best_val_wer:    0.9992977528089888
    final_val_cer:   0.7413491786088781
    training_time:   12.5 min

[Experiment 6] S06: Standard Enc (Char, Std, LSTM)
------------------------------------------------------------
  Model:
    token_type:      char
    encoder_type:    standard
    decoder_type:    lstm
  Training:
    learning_rate:   0.0005
    num_epochs:      100
    batch_size:      32
    dropout:         0.3
  Results:
    best_val_wer:    1.0014044943820224
    final_val_cer:   0.8271000815565653
    training_time:   23.0 min

[Experiment 5] S05: Low LR (1e-4)
------------------------------------------------------------
  Model:
    token_type:      char
    encoder_type:    pyramidal
    decoder_type:    lstm
  Training:
    learning_rate:   0.0001
    num_epochs:      100
    batch_size:      32
    dropout:         0.3
  Results:
    best_val_wer:    1.0926966292134832
    final_val_cer:   0.8379354538040312
    training_time:   12.8 min

