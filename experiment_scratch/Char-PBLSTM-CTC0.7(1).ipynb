{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee835d5",
   "metadata": {},
   "source": [
    "# Single Config Experiment Runner\n",
    "\n",
    "Use this notebook to run exactly one experiment definition declared directly in the cell below.\n",
    "It builds the dataloaders/model based on that config and prints the train/validation loss after each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "992bec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import replace\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "\n",
    "from config import Config\n",
    "from src.dataset import JavaneseASRDataset, collate_fn\n",
    "from src.features import LogMelFeatureExtractor\n",
    "from src.vocab import Vocabulary\n",
    "from src.decoder import GreedyDecoder\n",
    "from src.utils import set_seed, read_transcript, count_parameters\n",
    "from src.data_split import create_speaker_disjoint_split, load_split_info\n",
    "from src.model import Seq2SeqASR\n",
    "from scripts.train import train_one_epoch, validate_with_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55438cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected experiment: Inline: Char + CTC Joint\n",
      "Device: cuda\n",
      "Epochs: 25, batch_size: 4, lr: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Declare the single experiment directly here\n",
    "EXPERIMENT_7 = {\n",
    "    \"name\": \"Inline: Char + CTC Joint\",\n",
    "    \"description\": \"Character vocab with joint CTC-attention for alignment help.\",\n",
    "    \"config\": {\n",
    "        \"token_type\": \"char\",\n",
    "        \"encoder_type\": \"pyramidal\",\n",
    "        \"decoder_type\": \"lstm\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"num_epochs\": 25,\n",
    "        \"use_ctc\": True,\n",
    "        \"ctc_weight\": 0.7,\n",
    "        \"encoder_hidden_size\": 320,\n",
    "        \"decoder_dim\": 640,\n",
    "        \"batch_size\": 4\n",
    "    },\n",
    "}\n",
    "\n",
    "# Override options for quick tweaks\n",
    "MAX_EPOCHS = None       # set an int to cap runtime (e.g., 2 for a smoke test)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "base_config = Config()\n",
    "current_config = replace(base_config, **EXPERIMENT_7[\"config\"])\n",
    "if MAX_EPOCHS:\n",
    "    current_config = replace(current_config, num_epochs=int(MAX_EPOCHS))\n",
    "current_config = replace(current_config, device=DEVICE)\n",
    "\n",
    "print(f\"Selected experiment: {EXPERIMENT_7['name']}\")\n",
    "print(f\"Device: {current_config.device}\")\n",
    "print(f\"Epochs: {current_config.num_epochs}, batch_size: {current_config.batch_size}, lr: {current_config.learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0389a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloaders(cfg: Config):\n",
    "    transcripts = read_transcript(cfg.transcript_file)\n",
    "    vocab = Vocabulary(token_type=cfg.token_type)\n",
    "    vocab.build_from_transcripts(transcripts, min_freq=1)\n",
    "\n",
    "    feature_extractor = LogMelFeatureExtractor(\n",
    "        sample_rate=cfg.sample_rate,\n",
    "        n_mels=cfg.n_mels\n",
    "    )\n",
    "\n",
    "    split_info_path = Path(cfg.split_info_path)\n",
    "    if split_info_path.exists():\n",
    "        split_info = load_split_info(str(split_info_path))\n",
    "        split_dict = split_info[\"split\"]\n",
    "    else:\n",
    "        split_dict = create_speaker_disjoint_split(\n",
    "            transcript_file=cfg.transcript_file,\n",
    "            seed=cfg.seed,\n",
    "            save_split_info=True,\n",
    "            split_info_path=str(split_info_path)\n",
    "        )\n",
    "\n",
    "    train_dataset = JavaneseASRDataset(\n",
    "        audio_dir=cfg.audio_dir,\n",
    "        transcript_file=cfg.transcript_file,\n",
    "        vocab=vocab,\n",
    "        feature_extractor=feature_extractor,\n",
    "        apply_spec_augment=cfg.apply_spec_augment,\n",
    "        utt_id_filter=split_dict[\"train\"]\n",
    "    )\n",
    "\n",
    "    val_dataset = JavaneseASRDataset(\n",
    "        audio_dir=cfg.audio_dir,\n",
    "        transcript_file=cfg.transcript_file,\n",
    "        vocab=vocab,\n",
    "        feature_extractor=feature_extractor,\n",
    "        apply_spec_augment=False,\n",
    "        utt_id_filter=split_dict[\"val\"]\n",
    "    )\n",
    "\n",
    "    use_pin_memory = torch.cuda.is_available()\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=use_pin_memory,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=use_pin_memory,\n",
    "    )\n",
    "    return vocab, train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da2992bb-7e33-4ba9-97d9-a54735108c3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n",
      "Built char-level vocabulary with 34 tokens\n",
      "Audio file not found for utterance speaker46_f_nn_utt20\n",
      "Audio file not found for utterance speaker46_f_nn_utt21\n",
      "Audio file not found for utterance speaker46_f_nn_utt22\n",
      "Audio file not found for utterance speaker46_f_nn_utt23\n",
      "Audio file not found for utterance speaker46_f_nn_utt24\n",
      "Audio file not found for utterance speaker46_f_nn_utt25\n",
      "Audio file not found for utterance speaker46_f_nn_utt26\n",
      "Audio file not found for utterance speaker46_f_nn_utt27\n",
      "Audio file not found for utterance speaker46_f_nn_utt28\n",
      "Audio file not found for utterance speaker46_f_nn_utt29\n",
      "Filtered dataset: 2090 -> 1470 utterances\n",
      "Validating audio files...\n",
      "Loaded 1470 valid utterances from data/transcripts.csv\n",
      "Audio file not found for utterance speaker46_f_nn_utt20\n",
      "Audio file not found for utterance speaker46_f_nn_utt21\n",
      "Audio file not found for utterance speaker46_f_nn_utt22\n",
      "Audio file not found for utterance speaker46_f_nn_utt23\n",
      "Audio file not found for utterance speaker46_f_nn_utt24\n",
      "Audio file not found for utterance speaker46_f_nn_utt25\n",
      "Audio file not found for utterance speaker46_f_nn_utt26\n",
      "Audio file not found for utterance speaker46_f_nn_utt27\n",
      "Audio file not found for utterance speaker46_f_nn_utt28\n",
      "Audio file not found for utterance speaker46_f_nn_utt29\n",
      "Filtered dataset: 2090 -> 210 utterances\n",
      "Validating audio files...\n",
      "Loaded 210 valid utterances from data/transcripts.csv\n",
      "Vocabulary size: 34\n",
      "Train batches: 368, Val batches: 53\n",
      "Model parameters: 13,708,022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.25it/s, loss=1.8094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - train_loss: 2.4681 | val_loss: 2.7336 | val_cer: 4.9333 | val_wer: 4.2683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 368/368 [01:25<00:00,  4.29it/s, loss=1.7274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - train_loss: 1.8659 | val_loss: 2.3956 | val_cer: 3.5341 | val_wer: 3.2949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 368/368 [01:25<00:00,  4.30it/s, loss=1.2165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - train_loss: 1.6220 | val_loss: 2.2574 | val_cer: 4.1821 | val_wer: 4.1608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 368/368 [01:23<00:00,  4.41it/s, loss=1.2465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - train_loss: 1.4651 | val_loss: 2.0777 | val_cer: 1.2663 | val_wer: 1.7711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.27it/s, loss=1.4364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - train_loss: 1.3364 | val_loss: 1.9703 | val_cer: 0.9361 | val_wer: 1.3638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|██████████| 368/368 [01:25<00:00,  4.33it/s, loss=1.4745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - train_loss: 1.2357 | val_loss: 1.9170 | val_cer: 0.8874 | val_wer: 1.3385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.25it/s, loss=1.0741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - train_loss: 1.1411 | val_loss: 1.9397 | val_cer: 0.7509 | val_wer: 1.2647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.25it/s, loss=0.9412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - train_loss: 1.0952 | val_loss: 1.9386 | val_cer: 0.6466 | val_wer: 0.9930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|██████████| 368/368 [01:25<00:00,  4.30it/s, loss=0.4880]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - train_loss: 1.0199 | val_loss: 1.8038 | val_cer: 0.5257 | val_wer: 0.9298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.24it/s, loss=0.9531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - train_loss: 0.9830 | val_loss: 1.8081 | val_cer: 0.4550 | val_wer: 0.8708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.25it/s, loss=0.9540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - train_loss: 0.9209 | val_loss: 1.7912 | val_cer: 0.6759 | val_wer: 1.1313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.27it/s, loss=1.1613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - train_loss: 0.8957 | val_loss: 1.8060 | val_cer: 0.4909 | val_wer: 0.8904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Train]: 100%|██████████| 368/368 [01:27<00:00,  4.22it/s, loss=0.7237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - train_loss: 0.8497 | val_loss: 1.8332 | val_cer: 0.5959 | val_wer: 1.0534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.25it/s, loss=1.2706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - train_loss: 0.8244 | val_loss: 1.7570 | val_cer: 0.4111 | val_wer: 0.7781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Train]: 100%|██████████| 368/368 [01:27<00:00,  4.23it/s, loss=1.5721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - train_loss: 0.7999 | val_loss: 1.7672 | val_cer: 0.3914 | val_wer: 0.7430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [Train]: 100%|██████████| 368/368 [01:25<00:00,  4.31it/s, loss=0.6623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - train_loss: 0.7886 | val_loss: 1.7442 | val_cer: 0.4410 | val_wer: 0.8553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [Train]: 100%|██████████| 368/368 [01:27<00:00,  4.21it/s, loss=0.6016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - train_loss: 0.7448 | val_loss: 1.7339 | val_cer: 0.3663 | val_wer: 0.7669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.27it/s, loss=0.9033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - train_loss: 0.7295 | val_loss: 1.7637 | val_cer: 0.4633 | val_wer: 0.8483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [Train]: 100%|██████████| 368/368 [01:25<00:00,  4.32it/s, loss=0.6882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 - train_loss: 0.7332 | val_loss: 1.7460 | val_cer: 0.4239 | val_wer: 0.8167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 [Train]: 100%|██████████| 368/368 [01:27<00:00,  4.23it/s, loss=0.3103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 - train_loss: 0.7051 | val_loss: 1.7338 | val_cer: 0.4203 | val_wer: 0.7690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 [Train]: 100%|██████████| 368/368 [01:25<00:00,  4.29it/s, loss=0.2579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 - train_loss: 0.6855 | val_loss: 1.7046 | val_cer: 0.3712 | val_wer: 0.7577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 [Train]: 100%|██████████| 368/368 [01:27<00:00,  4.22it/s, loss=0.2504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 - train_loss: 0.6825 | val_loss: 1.7055 | val_cer: 0.3070 | val_wer: 0.6721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.24it/s, loss=0.6784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 - train_loss: 0.6658 | val_loss: 1.7032 | val_cer: 0.4245 | val_wer: 0.8497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.25it/s, loss=1.4464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 - train_loss: 0.6735 | val_loss: 1.6569 | val_cer: 0.2725 | val_wer: 0.6320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.25it/s, loss=0.8303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 - train_loss: 0.6647 | val_loss: 1.7126 | val_cer: 0.2957 | val_wer: 0.6735\n",
      "Restored best model from epoch 24 (val_loss=1.6569)\n"
     ]
    }
   ],
   "source": [
    "set_seed(current_config.seed)\n",
    "\n",
    "vocab, train_loader, val_loader = build_dataloaders(current_config)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "model = Seq2SeqASR(\n",
    "    vocab_size=len(vocab),\n",
    "    input_dim=current_config.input_dim,\n",
    "    encoder_hidden_size=current_config.encoder_hidden_size,\n",
    "    encoder_num_layers=current_config.encoder_num_layers,\n",
    "    decoder_dim=current_config.decoder_dim,\n",
    "    attention_dim=current_config.attention_dim,\n",
    "    embedding_dim=current_config.embedding_dim,\n",
    "    dropout=current_config.dropout,\n",
    "    use_ctc=current_config.use_ctc,\n",
    "    ctc_weight=current_config.ctc_weight,\n",
    "    encoder_type=current_config.encoder_type,\n",
    "    decoder_type=current_config.decoder_type,\n",
    ").to(current_config.device)\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=current_config.learning_rate)\n",
    "decoder = GreedyDecoder(model, vocab, max_len=current_config.max_decode_len, device=current_config.device)\n",
    "\n",
    "train_losses, val_losses, val_cers, val_wers = [], [], [], []\n",
    "\n",
    "patience = 5  # early stopping rounds\n",
    "bad_epochs = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state = None\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(1, current_config.num_epochs + 1):\n",
    "    train_loss = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        vocab,\n",
    "        current_config.device,\n",
    "        epoch,\n",
    "        current_config.grad_clip_norm,\n",
    "        encoder_type=current_config.encoder_type,\n",
    "    )\n",
    "    val_loss, val_cer, val_wer, _, _ = validate_with_metrics(\n",
    "        model,\n",
    "        val_loader,\n",
    "        decoder,\n",
    "        vocab,\n",
    "        current_config.device,\n",
    "        encoder_type=current_config.encoder_type,\n",
    "    )\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_cers.append(val_cer)\n",
    "    val_wers.append(val_wer)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{current_config.num_epochs} - \"\n",
    "        f\"train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | \"\n",
    "        f\"val_cer: {val_cer:.4f} | val_wer: {val_wer:.4f}\"\n",
    "    )\n",
    "\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_epoch = epoch\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        if bad_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no val_loss improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"Restored best model from epoch {best_epoch} (val_loss={best_val_loss:.4f})\")\n",
    "else:\n",
    "    print(\"No improvement tracked; using last epoch model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2da7b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Final train_loss: 0.6647\n",
      "Final val_loss: 1.7126\n",
      "Best val WER: 0.6320\n"
     ]
    }
   ],
   "source": [
    "print(\"Finished.\")\n",
    "if train_losses:\n",
    "    print(f\"Final train_loss: {train_losses[-1]:.4f}\")\n",
    "if val_losses:\n",
    "    print(f\"Final val_loss: {val_losses[-1]:.4f}\")\n",
    "if val_wers:\n",
    "    best_wer = min(val_wers)\n",
    "    print(f\"Best val WER: {best_wer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fefebccb-7b90-4ac3-8f0c-87b583fab49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file not found for utterance speaker46_f_nn_utt20\n",
      "Audio file not found for utterance speaker46_f_nn_utt21\n",
      "Audio file not found for utterance speaker46_f_nn_utt22\n",
      "Audio file not found for utterance speaker46_f_nn_utt23\n",
      "Audio file not found for utterance speaker46_f_nn_utt24\n",
      "Audio file not found for utterance speaker46_f_nn_utt25\n",
      "Audio file not found for utterance speaker46_f_nn_utt26\n",
      "Audio file not found for utterance speaker46_f_nn_utt27\n",
      "Audio file not found for utterance speaker46_f_nn_utt28\n",
      "Audio file not found for utterance speaker46_f_nn_utt29\n",
      "Filtered dataset: 2090 -> 410 utterances\n",
      "Validating audio files...\n",
      "Loaded 410 valid utterances from data/transcripts.csv\n",
      "Test utterances: 410; batches: 103\n",
      "Test avg_loss: 1.9075 | avg_cer: 0.3091 | avg_wer: 0.6773\n",
      "Random sample of test predictions:\n",
      "[speaker55_m_nn_utt08]REF: aku pengin turu rumiyin HYP: aku pengin turu pumiyin\n",
      "[speaker07_m_n_utt28]REF: preinan sesuk aku pengin dolan ning grojogan sewu HYP: rakine aku pengin tolan neng kerja kansan\n",
      "[speaker04_f_n_utt13]REF: ngrungoke lagu karo nggambar kuwi enak banget HYP: kuu mateh aku karo nggambarku yadak banget\n",
      "[speaker61_m_nn_utt30]REF: lawange omah iku dikunci saka njaba HYP: lawange omahiku dikunci sokkan iku ncisa kanca abu\n",
      "[speaker14_m_n_utt21]REF: listrike mati peteng kabeh HYP: wis cetem apik pepangapapa\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import jiwer\n",
    "from src.metrics import compute_batch_cer\n",
    "\n",
    "def build_test_loader(cfg: Config, vocab: Vocabulary):\n",
    "    split_info = load_split_info(cfg.split_info_path)\n",
    "    test_ids = split_info.get(\"split\", {}).get(\"test\", [])\n",
    "    if not test_ids:\n",
    "        raise ValueError(\"No test IDs found in split info; regenerate splits first.\")\n",
    "\n",
    "    feature_extractor = LogMelFeatureExtractor(\n",
    "        sample_rate=cfg.sample_rate,\n",
    "        n_mels=cfg.n_mels\n",
    "    )\n",
    "\n",
    "    test_dataset = JavaneseASRDataset(\n",
    "        audio_dir=cfg.audio_dir,\n",
    "        transcript_file=cfg.transcript_file,\n",
    "        vocab=vocab,\n",
    "        feature_extractor=feature_extractor,\n",
    "        apply_spec_augment=False,\n",
    "        utt_id_filter=test_ids,\n",
    "    )\n",
    "\n",
    "    use_pin_memory = torch.cuda.is_available()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=use_pin_memory,\n",
    "    )\n",
    "    return test_dataset, test_loader\n",
    "\n",
    "test_dataset, test_loader = build_test_loader(current_config, vocab)\n",
    "decoder_eval = GreedyDecoder(model, vocab, max_len=current_config.max_decode_len, device=current_config.device)\n",
    "\n",
    "print(f\"Test utterances: {len(test_dataset)}; batches: {len(test_loader)}\")\n",
    "\n",
    "model.eval()\n",
    "all_refs, all_hyps = [], []\n",
    "total_loss = 0.0\n",
    "total_cer = 0.0\n",
    "total_samples = 0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        features = batch[\"features\"].to(current_config.device)\n",
    "        feature_lengths = batch[\"feature_lengths\"].to(current_config.device)\n",
    "        targets = batch[\"targets\"].to(current_config.device)\n",
    "        target_lengths = batch[\"target_lengths\"].to(current_config.device)\n",
    "        transcripts = batch[\"transcripts\"]\n",
    "\n",
    "        attention_logits, ctc_logits = model(features, feature_lengths, targets, teacher_forcing_ratio=0.0)\n",
    "        encoder_lengths = feature_lengths // 4 if current_config.encoder_type == \"pyramidal\" else feature_lengths\n",
    "        loss = model.compute_loss(\n",
    "            attention_logits=attention_logits,\n",
    "            targets=targets,\n",
    "            target_lengths=target_lengths,\n",
    "            ctc_logits=ctc_logits,\n",
    "            encoder_lengths=encoder_lengths,\n",
    "            pad_idx=vocab.pad_idx,\n",
    "            blank_idx=vocab.blank_idx,\n",
    "        )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        hyps = decoder_eval.decode(features, feature_lengths)\n",
    "        cer = compute_batch_cer(transcripts, hyps)\n",
    "        total_cer += cer * len(transcripts)\n",
    "        total_samples += len(transcripts)\n",
    "        all_refs.extend(transcripts)\n",
    "        all_hyps.extend(hyps)\n",
    "\n",
    "avg_loss = total_loss / num_batches if num_batches else 0.0\n",
    "avg_cer = total_cer / total_samples if total_samples else 0.0\n",
    "avg_wer = jiwer.wer(all_refs, all_hyps) if all_refs else 0.0\n",
    "\n",
    "print(f\"Test avg_loss: {avg_loss:.4f} | avg_cer: {avg_cer:.4f} | avg_wer: {avg_wer:.4f}\")\n",
    "\n",
    "# Randomly sample 5 test utterances for inspection\n",
    "n_show = min(5, len(test_dataset))\n",
    "sample_indices = random.sample(range(len(test_dataset)), n_show) if n_show else []\n",
    "print(\"Random sample of test predictions:\")\n",
    "for idx in sample_indices:\n",
    "    feats, tgt, transcript, utt_id = test_dataset[idx]\n",
    "    feat_len = torch.tensor([feats.size(0)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        hyp = decoder_eval.decode(\n",
    "            feats.unsqueeze(0).to(current_config.device),\n",
    "            feat_len.to(current_config.device)\n",
    "        )[0]\n",
    "    print(f\"[{utt_id}]REF: {transcript} HYP: {hyp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
