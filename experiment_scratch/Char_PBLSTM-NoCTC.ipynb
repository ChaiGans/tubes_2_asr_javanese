{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee835d5",
   "metadata": {},
   "source": [
    "# Single Config Experiment Runner\n",
    "\n",
    "Use this notebook to run exactly one experiment definition declared directly in the cell below.\n",
    "It builds the dataloaders/model based on that config and prints the train/validation loss after each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "992bec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import replace\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "\n",
    "from config import Config\n",
    "from src.dataset import JavaneseASRDataset, collate_fn\n",
    "from src.features import LogMelFeatureExtractor\n",
    "from src.vocab import Vocabulary\n",
    "from src.decoder import GreedyDecoder\n",
    "from src.utils import set_seed, read_transcript, count_parameters\n",
    "from src.data_split import create_speaker_disjoint_split, load_split_info\n",
    "from src.model import Seq2SeqASR\n",
    "from scripts.train import train_one_epoch, validate_with_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55438cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected experiment: Inline: Char + CTC Joint\n",
      "Device: cuda\n",
      "Epochs: 25, batch_size: 4, lr: 0.001\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_2 = {\n",
    "    \"name\": \"Char-Standard-NoCTC\",\n",
    "    \"description\": \"Baseline char model without CTC loss.\",\n",
    "    \"config\": {\n",
    "        \"token_type\": \"char\",\n",
    "        \"encoder_type\": \"pyramidal\",\n",
    "        \"decoder_type\": \"lstm\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"num_epochs\": 25,\n",
    "        \"use_ctc\": False,\n",
    "        \"ctc_weight\": 0.0,\n",
    "        \"encoder_hidden_size\": 128,\n",
    "        \"decoder_dim\": 256,\n",
    "        \"batch_size\": 4\n",
    "    },\n",
    "}\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "base_config = Config()\n",
    "current_config = replace(base_config, **EXPERIMENT_2[\"config\"])\n",
    "current_config = replace(current_config, device=DEVICE)\n",
    "\n",
    "print(f\"Selected experiment: {EXPERIMENT_2['name']}\")\n",
    "print(f\"Device: {current_config.device}\")\n",
    "print(f\"Epochs: {current_config.num_epochs}, batch_size: {current_config.batch_size}, lr: {current_config.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0389a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloaders(cfg: Config):\n",
    "    transcripts = read_transcript(cfg.transcript_file)\n",
    "    vocab = Vocabulary(token_type=cfg.token_type)\n",
    "    vocab.build_from_transcripts(transcripts, min_freq=1)\n",
    "\n",
    "    feature_extractor = LogMelFeatureExtractor(\n",
    "        sample_rate=cfg.sample_rate,\n",
    "        n_mels=cfg.n_mels\n",
    "    )\n",
    "\n",
    "    split_info_path = Path(cfg.split_info_path)\n",
    "    if split_info_path.exists():\n",
    "        split_info = load_split_info(str(split_info_path))\n",
    "        split_dict = split_info[\"split\"]\n",
    "    else:\n",
    "        split_dict = create_speaker_disjoint_split(\n",
    "            transcript_file=cfg.transcript_file,\n",
    "            seed=cfg.seed,\n",
    "            save_split_info=True,\n",
    "            split_info_path=str(split_info_path)\n",
    "        )\n",
    "\n",
    "    train_dataset = JavaneseASRDataset(\n",
    "        audio_dir=cfg.audio_dir,\n",
    "        transcript_file=cfg.transcript_file,\n",
    "        vocab=vocab,\n",
    "        feature_extractor=feature_extractor,\n",
    "        apply_spec_augment=cfg.apply_spec_augment,\n",
    "        utt_id_filter=split_dict[\"train\"]\n",
    "    )\n",
    "\n",
    "    val_dataset = JavaneseASRDataset(\n",
    "        audio_dir=cfg.audio_dir,\n",
    "        transcript_file=cfg.transcript_file,\n",
    "        vocab=vocab,\n",
    "        feature_extractor=feature_extractor,\n",
    "        apply_spec_augment=False,\n",
    "        utt_id_filter=split_dict[\"val\"]\n",
    "    )\n",
    "\n",
    "    use_pin_memory = torch.cuda.is_available()\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=use_pin_memory,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=use_pin_memory,\n",
    "    )\n",
    "    return vocab, train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2992bb-7e33-4ba9-97d9-a54735108c3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n",
      "Built char-level vocabulary with 34 tokens\n",
      "Audio file not found for utterance speaker46_f_nn_utt20\n",
      "Audio file not found for utterance speaker46_f_nn_utt21\n",
      "Audio file not found for utterance speaker46_f_nn_utt22\n",
      "Audio file not found for utterance speaker46_f_nn_utt23\n",
      "Audio file not found for utterance speaker46_f_nn_utt24\n",
      "Audio file not found for utterance speaker46_f_nn_utt25\n",
      "Audio file not found for utterance speaker46_f_nn_utt26\n",
      "Audio file not found for utterance speaker46_f_nn_utt27\n",
      "Audio file not found for utterance speaker46_f_nn_utt28\n",
      "Audio file not found for utterance speaker46_f_nn_utt29\n",
      "Filtered dataset: 2090 -> 1470 utterances\n",
      "Validating audio files...\n",
      "Loaded 1470 valid utterances from data/transcripts.csv\n",
      "Audio file not found for utterance speaker46_f_nn_utt20\n",
      "Audio file not found for utterance speaker46_f_nn_utt21\n",
      "Audio file not found for utterance speaker46_f_nn_utt22\n",
      "Audio file not found for utterance speaker46_f_nn_utt23\n",
      "Audio file not found for utterance speaker46_f_nn_utt24\n",
      "Audio file not found for utterance speaker46_f_nn_utt25\n",
      "Audio file not found for utterance speaker46_f_nn_utt26\n",
      "Audio file not found for utterance speaker46_f_nn_utt27\n",
      "Audio file not found for utterance speaker46_f_nn_utt28\n",
      "Audio file not found for utterance speaker46_f_nn_utt29\n",
      "Filtered dataset: 2090 -> 210 utterances\n",
      "Validating audio files...\n",
      "Loaded 210 valid utterances from data/transcripts.csv\n",
      "Vocabulary size: 34\n",
      "Train batches: 368, Val batches: 53\n",
      "Model parameters: 13,708,022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 368/368 [01:34<00:00,  3.91it/s, loss=1.7904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - train_loss: 2.3641 | val_loss: 3.2580 | val_cer: 4.7635 | val_wer: 3.9333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 368/368 [01:34<00:00,  3.91it/s, loss=1.6524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - train_loss: 1.7956 | val_loss: 3.0676 | val_cer: 4.7229 | val_wer: 4.0660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 368/368 [01:34<00:00,  3.89it/s, loss=1.2078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - train_loss: 1.5543 | val_loss: 2.9659 | val_cer: 2.7929 | val_wer: 3.2978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 368/368 [01:30<00:00,  4.08it/s, loss=1.1433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - train_loss: 1.3803 | val_loss: 2.7254 | val_cer: 1.1673 | val_wer: 1.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 368/368 [01:27<00:00,  4.21it/s, loss=1.3270]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - train_loss: 1.2431 | val_loss: 2.6681 | val_cer: 1.4605 | val_wer: 1.9782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|██████████| 368/368 [01:34<00:00,  3.88it/s, loss=1.2916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - train_loss: 1.1238 | val_loss: 2.5826 | val_cer: 0.8370 | val_wer: 1.2886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|██████████| 368/368 [01:35<00:00,  3.85it/s, loss=0.9407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - train_loss: 1.0255 | val_loss: 2.5567 | val_cer: 0.7447 | val_wer: 1.2381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|██████████| 368/368 [01:35<00:00,  3.85it/s, loss=0.8869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - train_loss: 0.9595 | val_loss: 2.6149 | val_cer: 0.7087 | val_wer: 1.0969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|██████████| 368/368 [01:35<00:00,  3.85it/s, loss=0.5028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - train_loss: 0.9054 | val_loss: 2.4910 | val_cer: 0.5250 | val_wer: 0.9199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]: 100%|██████████| 368/368 [01:27<00:00,  4.22it/s, loss=0.6834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - train_loss: 0.8466 | val_loss: 2.5155 | val_cer: 0.4469 | val_wer: 0.8202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Train]: 100%|██████████| 368/368 [01:27<00:00,  4.21it/s, loss=0.8741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - train_loss: 0.8015 | val_loss: 2.4337 | val_cer: 0.3756 | val_wer: 0.7886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Train]: 100%|██████████| 368/368 [01:38<00:00,  3.72it/s, loss=1.0590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - train_loss: 0.8085 | val_loss: 2.4270 | val_cer: 0.5226 | val_wer: 0.8926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Train]: 100%|██████████| 368/368 [01:38<00:00,  3.73it/s, loss=0.6087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - train_loss: 0.7811 | val_loss: 2.4232 | val_cer: 0.5202 | val_wer: 0.9249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Train]: 100%|██████████| 368/368 [01:38<00:00,  3.74it/s, loss=1.0674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - train_loss: 0.7250 | val_loss: 2.3991 | val_cer: 0.4146 | val_wer: 0.8013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Train]: 100%|██████████| 368/368 [01:36<00:00,  3.81it/s, loss=1.4388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - train_loss: 0.6915 | val_loss: 2.3223 | val_cer: 0.4214 | val_wer: 0.8048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.27it/s, loss=0.4828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - train_loss: 0.6755 | val_loss: 2.4761 | val_cer: 0.4343 | val_wer: 0.7985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [Train]: 100%|██████████| 368/368 [01:27<00:00,  4.20it/s, loss=0.5972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - train_loss: 0.6376 | val_loss: 2.4519 | val_cer: 0.3085 | val_wer: 0.6419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.23it/s, loss=0.7590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - train_loss: 0.6542 | val_loss: 2.3329 | val_cer: 0.3439 | val_wer: 0.7282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [Train]: 100%|██████████| 368/368 [01:27<00:00,  4.22it/s, loss=0.6670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 - train_loss: 0.6493 | val_loss: 2.3394 | val_cer: 0.4483 | val_wer: 0.8287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.24it/s, loss=0.2759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 - train_loss: 0.6638 | val_loss: 2.2683 | val_cer: 0.4324 | val_wer: 0.8272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.24it/s, loss=0.2878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 - train_loss: 0.6169 | val_loss: 2.3544 | val_cer: 0.3133 | val_wer: 0.7044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.26it/s, loss=0.2410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 - train_loss: 0.6117 | val_loss: 2.2553 | val_cer: 0.4165 | val_wer: 0.7935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.26it/s, loss=0.4333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 - train_loss: 0.5961 | val_loss: 2.2744 | val_cer: 0.4107 | val_wer: 0.7591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.27it/s, loss=1.2969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 - train_loss: 0.5833 | val_loss: 2.3263 | val_cer: 0.3460 | val_wer: 0.7135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 [Train]: 100%|██████████| 368/368 [01:26<00:00,  4.27it/s, loss=0.8940]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 - train_loss: 0.5977 | val_loss: 2.2980 | val_cer: 0.3267 | val_wer: 0.6889\n",
      "Restored best model from epoch 22 (val_loss=2.2553)\n"
     ]
    }
   ],
   "source": [
    "set_seed(current_config.seed)\n",
    "\n",
    "vocab, train_loader, val_loader = build_dataloaders(current_config)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "model = Seq2SeqASR(\n",
    "    vocab_size=len(vocab),\n",
    "    input_dim=current_config.input_dim,\n",
    "    encoder_hidden_size=current_config.encoder_hidden_size,\n",
    "    encoder_num_layers=current_config.encoder_num_layers,\n",
    "    decoder_dim=current_config.decoder_dim,\n",
    "    attention_dim=current_config.attention_dim,\n",
    "    embedding_dim=current_config.embedding_dim,\n",
    "    dropout=current_config.dropout,\n",
    "    use_ctc=current_config.use_ctc,\n",
    "    ctc_weight=current_config.ctc_weight,\n",
    "    encoder_type=current_config.encoder_type,\n",
    "    decoder_type=current_config.decoder_type,\n",
    ").to(current_config.device)\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=current_config.learning_rate)\n",
    "decoder = GreedyDecoder(model, vocab, max_len=current_config.max_decode_len, device=current_config.device)\n",
    "\n",
    "train_losses, val_losses, val_cers, val_wers = [], [], [], []\n",
    "\n",
    "patience = 5  # early stopping rounds\n",
    "bad_epochs = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state = None\n",
    "best_epoch = 0\n",
    "\n",
    "import time\n",
    "\n",
    "total_start = time.time() \n",
    "\n",
    "for epoch in range(1, current_config.num_epochs + 1):\n",
    "    train_loss = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        vocab,\n",
    "        current_config.device,\n",
    "        epoch,\n",
    "        current_config.grad_clip_norm,\n",
    "        encoder_type=current_config.encoder_type,\n",
    "    )\n",
    "    val_loss, val_cer, val_wer, _, _ = validate_with_metrics(\n",
    "        model,\n",
    "        val_loader,\n",
    "        decoder,\n",
    "        vocab,\n",
    "        current_config.device,\n",
    "        encoder_type=current_config.encoder_type,\n",
    "    )\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_cers.append(val_cer)\n",
    "    val_wers.append(val_wer)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{current_config.num_epochs} - \"\n",
    "        f\"train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | \"\n",
    "        f\"val_cer: {val_cer:.4f} | val_wer: {val_wer:.4f}\"\n",
    "    )\n",
    "\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        best_epoch = epoch\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        if bad_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no val_loss improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\nTOTAL TRAINING TIME: {total_time/60:.2f} minutes ({total_time:.2f} seconds)\")\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"Restored best model from epoch {best_epoch} (val_loss={best_val_loss:.4f})\")\n",
    "else:\n",
    "    print(\"No improvement tracked; using last epoch model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da7b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Final train_loss: 0.5977\n",
      "Final val_loss: 2.2980\n",
      "Best val WER: 0.6419\n"
     ]
    }
   ],
   "source": [
    "print(\"Finished.\")\n",
    "if train_losses:\n",
    "    print(f\"Final train_loss: {train_losses[-1]:.4f}\")\n",
    "if val_losses:\n",
    "    print(f\"Final val_loss: {val_losses[-1]:.4f}\")\n",
    "if val_wers:\n",
    "    best_wer = min(val_wers)\n",
    "    print(f\"Best val WER: {best_wer:.4f}\")\n",
    "print(f\"\\nTOTAL TRAINING TIME: {total_time/60:.2f} minutes ({total_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80698e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "notebook_name = os.path.splitext(os.path.basename(sys.argv[0]))[0]\n",
    "save_name = f\"{notebook_name}_loss_graph.png\"\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o')\n",
    "plt.plot(epochs, val_losses, label=\"Validation Loss\", marker='s')\n",
    "\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(save_name)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved loss graph to: {save_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fefebccb-7b90-4ac3-8f0c-87b583fab49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file not found for utterance speaker46_f_nn_utt20\n",
      "Audio file not found for utterance speaker46_f_nn_utt21\n",
      "Audio file not found for utterance speaker46_f_nn_utt22\n",
      "Audio file not found for utterance speaker46_f_nn_utt23\n",
      "Audio file not found for utterance speaker46_f_nn_utt24\n",
      "Audio file not found for utterance speaker46_f_nn_utt25\n",
      "Audio file not found for utterance speaker46_f_nn_utt26\n",
      "Audio file not found for utterance speaker46_f_nn_utt27\n",
      "Audio file not found for utterance speaker46_f_nn_utt28\n",
      "Audio file not found for utterance speaker46_f_nn_utt29\n",
      "Filtered dataset: 2090 -> 410 utterances\n",
      "Validating audio files...\n",
      "Loaded 410 valid utterances from data/transcripts.csv\n",
      "Test utterances: 410; batches: 103\n",
      "Test avg_loss: 2.4820 | avg_cer: 0.3437 | avg_wer: 0.7191\n",
      "Random sample of test predictions:\n",
      "[speaker55_m_nn_utt08]REF: aku pengin turu rumiyin HYP: aku pengin turu pumiyin\n",
      "[speaker07_m_n_utt28]REF: preinan sesuk aku pengin dolan ning grojogan sewu HYP: pak eri nang seso aku pengin tulan neng kerjo kan susu\n",
      "[speaker04_f_n_utt13]REF: ngrungoke lagu karo nggambar kuwi enak banget HYP: runga aku arep gambar kuyana banget\n",
      "[speaker61_m_nn_utt30]REF: lawange omah iku dikunci saka njaba HYP: wah wangi uumai kula digun cisa kaya njapu\n",
      "[speaker14_m_n_utt21]REF: listrike mati peteng kabeh HYP: lija jam ati paton kabeh\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import jiwer\n",
    "from src.metrics import compute_batch_cer\n",
    "\n",
    "def build_test_loader(cfg: Config, vocab: Vocabulary):\n",
    "    split_info = load_split_info(cfg.split_info_path)\n",
    "    test_ids = split_info.get(\"split\", {}).get(\"test\", [])\n",
    "    if not test_ids:\n",
    "        raise ValueError(\"No test IDs found in split info; regenerate splits first.\")\n",
    "\n",
    "    feature_extractor = LogMelFeatureExtractor(\n",
    "        sample_rate=cfg.sample_rate,\n",
    "        n_mels=cfg.n_mels\n",
    "    )\n",
    "\n",
    "    test_dataset = JavaneseASRDataset(\n",
    "        audio_dir=cfg.audio_dir,\n",
    "        transcript_file=cfg.transcript_file,\n",
    "        vocab=vocab,\n",
    "        feature_extractor=feature_extractor,\n",
    "        apply_spec_augment=False,\n",
    "        utt_id_filter=test_ids,\n",
    "    )\n",
    "\n",
    "    use_pin_memory = torch.cuda.is_available()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=use_pin_memory,\n",
    "    )\n",
    "    return test_dataset, test_loader\n",
    "\n",
    "test_dataset, test_loader = build_test_loader(current_config, vocab)\n",
    "decoder_eval = GreedyDecoder(model, vocab, max_len=current_config.max_decode_len, device=current_config.device)\n",
    "\n",
    "print(f\"Test utterances: {len(test_dataset)}; batches: {len(test_loader)}\")\n",
    "\n",
    "model.eval()\n",
    "all_refs, all_hyps = [], []\n",
    "total_loss = 0.0\n",
    "total_cer = 0.0\n",
    "total_samples = 0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        features = batch[\"features\"].to(current_config.device)\n",
    "        feature_lengths = batch[\"feature_lengths\"].to(current_config.device)\n",
    "        targets = batch[\"targets\"].to(current_config.device)\n",
    "        target_lengths = batch[\"target_lengths\"].to(current_config.device)\n",
    "        transcripts = batch[\"transcripts\"]\n",
    "\n",
    "        attention_logits, ctc_logits = model(features, feature_lengths, targets, teacher_forcing_ratio=0.0)\n",
    "        encoder_lengths = feature_lengths // 4 if current_config.encoder_type == \"pyramidal\" else feature_lengths\n",
    "        loss = model.compute_loss(\n",
    "            attention_logits=attention_logits,\n",
    "            targets=targets,\n",
    "            target_lengths=target_lengths,\n",
    "            ctc_logits=ctc_logits,\n",
    "            encoder_lengths=encoder_lengths,\n",
    "            pad_idx=vocab.pad_idx,\n",
    "            blank_idx=vocab.blank_idx,\n",
    "        )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        hyps = decoder_eval.decode(features, feature_lengths)\n",
    "        cer = compute_batch_cer(transcripts, hyps)\n",
    "        total_cer += cer * len(transcripts)\n",
    "        total_samples += len(transcripts)\n",
    "        all_refs.extend(transcripts)\n",
    "        all_hyps.extend(hyps)\n",
    "\n",
    "avg_loss = total_loss / num_batches if num_batches else 0.0\n",
    "avg_cer = total_cer / total_samples if total_samples else 0.0\n",
    "avg_wer = jiwer.wer(all_refs, all_hyps) if all_refs else 0.0\n",
    "\n",
    "print(f\"Test avg_loss: {avg_loss:.4f} | avg_cer: {avg_cer:.4f} | avg_wer: {avg_wer:.4f}\")\n",
    "\n",
    "# Randomly sample 5 test utterances for inspection\n",
    "n_show = min(5, len(test_dataset))\n",
    "sample_indices = random.sample(range(len(test_dataset)), n_show) if n_show else []\n",
    "print(\"Random sample of test predictions:\")\n",
    "for idx in sample_indices:\n",
    "    feats, tgt, transcript, utt_id = test_dataset[idx]\n",
    "    feat_len = torch.tensor([feats.size(0)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        hyp = decoder_eval.decode(\n",
    "            feats.unsqueeze(0).to(current_config.device),\n",
    "            feat_len.to(current_config.device)\n",
    "        )[0]\n",
    "    print(f\"[{utt_id}]REF: {transcript} HYP: {hyp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
