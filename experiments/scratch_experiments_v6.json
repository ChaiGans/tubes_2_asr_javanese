{
  "name": "Scratch Model Expansion v6",
  "description": "Focus on the strongest setting so far (word vocab + GRU / larger capacity); combine and probe CTC and depth.",
  "created_at": "2025-12-06",
  "experiments": [
    {
      "name": "V6-01: Word + GRU + Larger Model",
      "description": "Combine best WER configs: word vocab, GRU decoder, larger encoder/decoder/attention dims.",
      "config": {
        "token_type": "word",
        "encoder_type": "pyramidal",
        "decoder_type": "gru",
        "encoder_hidden_size": 256,
        "decoder_dim": 512,
        "attention_dim": 256,
        "learning_rate": 5e-4,
        "num_epochs": 120
      }
    },
    {
      "name": "V6-02: Word + GRU + CTC Joint",
      "description": "Word vocab with GRU decoder and joint CTC to encourage alignment.",
      "config": {
        "token_type": "word",
        "encoder_type": "pyramidal",
        "decoder_type": "gru",
        "learning_rate": 5e-4,
        "num_epochs": 120,
        "use_ctc": true,
        "ctc_weight": 0.3
      }
    },
    {
      "name": "V6-03: Word + GRU + Deeper Encoder",
      "description": "Increase encoder depth to 4 layers (pyramidal) with GRU decoder.",
      "config": {
        "token_type": "word",
        "encoder_type": "pyramidal",
        "decoder_type": "gru",
        "encoder_num_layers": 4,
        "learning_rate": 5e-4,
        "num_epochs": 120
      }
    },
    {
      "name": "V6-04: Word + GRU + Larger + Low Dropout",
      "description": "Larger GRU model with reduced dropout to see if capacity is under-regularized.",
      "config": {
        "token_type": "word",
        "encoder_type": "pyramidal",
        "decoder_type": "gru",
        "encoder_hidden_size": 256,
        "decoder_dim": 512,
        "attention_dim": 256,
        "dropout": 0.1,
        "learning_rate": 5e-4,
        "num_epochs": 120
      }
    },
    {
      "name": "V6-05: Word + GRU + Standard Encoder",
      "description": "Ablation: remove pyramidal reduction (standard encoder) with GRU decoder.",
      "config": {
        "token_type": "word",
        "encoder_type": "standard",
        "decoder_type": "gru",
        "learning_rate": 5e-4,
        "num_epochs": 120
      }
    }
  ]
}
